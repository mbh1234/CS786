{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "me4Pe3-xssQt",
    "outputId": "3e70c1b7-ab2e-4b25-f271-a6ecd4569b0c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.0+cu121)\n",
      "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (1.0.11)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.2)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.24.7)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.5)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision timm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1) --> Design steps\n",
    "\n",
    "### --> TASKS -->\n",
    "\n",
    "There are three types of visual recognition/classification tasks are studied in the following\n",
    "sections. The first one is based on the concept of “symmetry” (section 4), the second\n",
    "one is based on “counting” (section 5), and the last one is based on “grouping or conformance behavior” (section 6). First two tasks include several sub-tasks, which may require additional learning of concepts such as “uniformity” or “grouping”. \n",
    "\n",
    "All tasks are designed as binary classification problems.\n",
    "\n",
    "### --> DATASET GENERATION -->\n",
    "\n",
    "We create synthetic data sets for these image recognition problems. All images are\n",
    "generated in size of 200 × 200.\n",
    "\n",
    "### --> NETWORK CONFIG -->\n",
    "\n",
    "To handle the binary tasks and these synthetic images, we adapt the inception V4\n",
    "model by changing the input size, and replacing the original softmax output layer by\n",
    "one hidden fully connected layer of 1024 nodes (with relu activation) plus one new\n",
    "softmax layer of 2 nodes. \n",
    "\n",
    "We also ensure all previous layers are frozen to retain the learned features from ImageNet, except the new layers (which will be trained)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 853,
     "referenced_widgets": [
      "7e73f282631b42c59e1483b65187399e",
      "8970696ebd684e519ee031f4ad75a8dd",
      "bd8cf7f6128849e895d4084d6c669add",
      "0a6afa0d17764312a4f24c781b5748a5",
      "16191e415f6c43a19eab62d25ef4d259",
      "034a20511b174f39a2e69863015cdf01",
      "4419c8fd9caa4c91888055adc29dfbcb",
      "4229910ccd614a42b9176565f9ec473f",
      "ce44b7e0ab4043a8be30b0496f16b5fb",
      "e14eeb99a50b465c80a17436a786e48f",
      "8bb28333ea744f2b94e5667ba58b92f7"
     ]
    },
    "id": "3IHogXrGF0ae",
    "outputId": "b59def95-0a58-4832-cc8f-52f549f2a7ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Symmetry A1,B1\n",
      "Using device: cuda\n",
      "{'0': 0, '1': 1}\n",
      "Total images: 8000\n",
      "Training images: 6400\n",
      "Validation images: 1600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e73f282631b42c59e1483b65187399e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/171M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 160/160 [03:00<00:00,  1.13s/it, loss=0.0049, acc=93.8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1302, Acc: 93.84%, Precision: 0.95, Recall: 0.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0234, Acc: 99.31%, Precision: 0.99, Recall: 1.00\n",
      "Saved best model with validation accuracy: 99.31%\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 160/160 [03:01<00:00,  1.14s/it, loss=0.00286, acc=99.6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0150, Acc: 99.58%, Precision: 1.00, Recall: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0039, Acc: 99.81%, Precision: 1.00, Recall: 1.00\n",
      "Saved best model with validation accuracy: 99.81%\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 160/160 [03:01<00:00,  1.13s/it, loss=0.192, acc=99.8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0089, Acc: 99.77%, Precision: 1.00, Recall: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0442, Acc: 98.50%, Precision: 0.98, Recall: 0.99\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 160/160 [03:03<00:00,  1.15s/it, loss=0.000117, acc=99.9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0049, Acc: 99.89%, Precision: 1.00, Recall: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0007, Acc: 100.00%, Precision: 1.00, Recall: 1.00\n",
      "Saved best model with validation accuracy: 100.00%\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 160/160 [03:02<00:00,  1.14s/it, loss=0.063, acc=99.5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0156, Acc: 99.55%, Precision: 0.99, Recall: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0192, Acc: 99.44%, Precision: 0.99, Recall: 1.00\n",
      "Saved final model to models/incep4_final.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import timm\n",
    "from tqdm import tqdm\n",
    "from torch.optim import RMSprop\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "class ImageFolderDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(os.listdir(root_dir))\n",
    "        self.class_to_idx = {cls: int(cls) for cls in self.classes}\n",
    "        print(self.class_to_idx)\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "        for class_name in self.classes:\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            class_idx = self.class_to_idx[class_name]\n",
    "\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                img_path = os.path.join(class_dir, img_name)\n",
    "                if img_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    self.images.append(img_path)\n",
    "                    self.labels.append(class_idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "class GaussianNoise:\n",
    "    def __init__(self, mean=0., std=0.1):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, img):\n",
    "        noise = torch.randn(img.size()) * self.std + self.mean\n",
    "        noisy_img = img + noise\n",
    "        return torch.clamp(noisy_img, 0., 1.)\n",
    "\n",
    "class SaltPepperNoise:\n",
    "    def __init__(self, prob=0.004):\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, img):\n",
    "        noisy_img = img.clone()\n",
    "        salt = torch.rand(img.size()) < (self.prob / 2)\n",
    "        pepper = torch.rand(img.size()) < (self.prob / 2)\n",
    "        noisy_img[salt] = 1\n",
    "        noisy_img[pepper] = 0\n",
    "        return noisy_img\n",
    "\n",
    "class RandomNoise:\n",
    "    def __init__(self, gaussian_std=0.1, sp_prob=0.004):\n",
    "        self.gaussian = GaussianNoise(std=gaussian_std)\n",
    "        self.salt_pepper = SaltPepperNoise(prob=sp_prob)\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if np.random.rand() > 0.4:\n",
    "            if np.random.rand() > 0.5:\n",
    "                return self.gaussian(img)\n",
    "            else:\n",
    "                return self.salt_pepper(img)\n",
    "        return img\n",
    "\n",
    "def build_model(num_classes=2, pretrained=True):\n",
    "    model = timm.create_model('inception_v4', pretrained=pretrained, num_classes=num_classes)\n",
    "    in_features = model.get_classifier().in_features\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Linear(in_features, 1024),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(1024, num_classes),\n",
    "        nn.Softmax(dim=1)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    pbar = tqdm(dataloader, desc='Training')\n",
    "    for inputs, labels in pbar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "        pbar.set_postfix({'loss': loss.item(), 'acc': 100.*correct/total})\n",
    "\n",
    "    precision = precision_score(all_labels, all_preds, average='binary')\n",
    "    recall = recall_score(all_labels, all_preds, average='binary')\n",
    "\n",
    "    return total_loss / total, 100.*correct/total, precision, recall\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "    precision = precision_score(all_labels, all_preds, average='binary')\n",
    "    recall = recall_score(all_labels, all_preds, average='binary')\n",
    "\n",
    "    return total_loss / total, 100.*correct/total, precision, recall\n",
    "\n",
    "def train_model(\n",
    "    data_dir,\n",
    "    valid_split=0.2,\n",
    "    resume_model_path=None,\n",
    "    enable_image_noise=False,\n",
    "    num_epochs=70,\n",
    "    batch_size=40,\n",
    "    learning_rate=0.0001\n",
    "):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.Resize((299, 299)),\n",
    "        transforms.ToTensor(),\n",
    "        RandomNoise() if enable_image_noise else transforms.Lambda(lambda x: x),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomRotation(5),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    valid_transforms = transforms.Compose([\n",
    "        transforms.Resize((299, 299)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    full_dataset = ImageFolderDataset(data_dir, transform=train_transforms)\n",
    "\n",
    "    valid_size = int(valid_split * len(full_dataset))\n",
    "    train_size = len(full_dataset) - valid_size\n",
    "\n",
    "    train_dataset, valid_dataset = random_split(\n",
    "        full_dataset,\n",
    "        [train_size, valid_size],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "\n",
    "    valid_dataset.dataset.transform = valid_transforms\n",
    "\n",
    "    print(f\"Total images: {len(full_dataset)}\")\n",
    "    print(f\"Training images: {len(train_dataset)}\")\n",
    "    print(f\"Validation images: {len(valid_dataset)}\")\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    model = build_model(num_classes=2)\n",
    "    model = model.to(device)\n",
    "\n",
    "    if resume_model_path is not None:\n",
    "        print(f'Resuming from checkpoint: {resume_model_path}')\n",
    "        model.load_state_dict(torch.load(resume_model_path))\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = RMSprop(model.parameters(), lr=learning_rate)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=5, verbose=True)\n",
    "\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "\n",
    "    best_val_acc = 0\n",
    "    best_model_path = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "\n",
    "        train_loss, train_acc, train_precision, train_recall = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        print(f'Training Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%, Precision: {train_precision:.2f}, Recall: {train_recall:.2f}')\n",
    "\n",
    "        val_loss, val_acc, val_precision, val_recall = validate(model, valid_loader, criterion, device)\n",
    "        print(f'Validation Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%, Precision: {val_precision:.2f}, Recall: {val_recall:.2f}')\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_path = f'models/incep4_best.pth'\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f'Saved best model with validation accuracy: {val_acc:.2f}%')\n",
    "\n",
    "        if optimizer.param_groups[0]['lr'] < 1e-6:\n",
    "            print('Learning rate too small. Stopping training.')\n",
    "            break\n",
    "\n",
    "    final_path = f'models/incep4_final.pth'\n",
    "    torch.save(model.state_dict(), final_path)\n",
    "    print(f'Saved final model to {final_path}')\n",
    "\n",
    "    return {\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'best_model_path': best_model_path,\n",
    "        'final_model_path': final_path\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Global Symmetry A1,B1\")\n",
    "    results = train_model(\n",
    "        data_dir='/content/ds1/train',\n",
    "        valid_split=0.2,\n",
    "        enable_image_noise=True,\n",
    "        num_epochs=5,\n",
    "        batch_size=40,\n",
    "        learning_rate=0.0001\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "LT_a8JCiywNU",
    "outputId": "a9c80ee5-a1ea-43b7-bfc1-5e5f8bb8a33b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Symmetry D(A1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-6bf068d7d83f>:52: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.78\n",
      "Test Precision: 0.97\n",
      "Test Recall: 0.59\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from PIL import Image\n",
    "import os\n",
    "import timm\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "class ImageFolderDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(os.listdir(root_dir))\n",
    "        self.class_to_idx = {cls: int(cls) for cls in self.classes}\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "        for class_name in self.classes:\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            class_idx = self.class_to_idx[class_name]\n",
    "\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                img_path = os.path.join(class_dir, img_name)\n",
    "                if img_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    self.images.append(img_path)\n",
    "                    self.labels.append(class_idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "def load_model(num_classes=2, model_path='models/incep4_best.pth'):\n",
    "    model = timm.create_model('inception_v4', pretrained=False, num_classes=num_classes)\n",
    "    in_features = model.get_classifier().in_features\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Linear(in_features, 1024),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(1024, num_classes),\n",
    "        nn.Softmax(dim=1)\n",
    "    )\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def test_model(model, dataloader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='binary')\n",
    "    recall = recall_score(all_labels, all_preds, average='binary')\n",
    "\n",
    "    print(f'Test Accuracy: {accuracy:.2f}')\n",
    "    print(f'Test Precision: {precision:.2f}')\n",
    "    print(f'Test Recall: {recall:.2f}')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_dir = '/content/ds1/train_break'  # Path to test dataset\n",
    "    print(\"Global Symmetry D(A1)\")\n",
    "    test_transforms = transforms.Compose([\n",
    "        transforms.Resize((299, 299)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    test_dataset = ImageFolderDataset(root_dir=test_dir, transform=test_transforms)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=40, shuffle=False, num_workers=2)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = load_model(num_classes=2, model_path='models/incep4_best.pth')\n",
    "    test_model(model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acknowledgment:  \n",
    "    This assignment is collaboratively done by:   \n",
    "    Keerthana - 210290  \n",
    "    Meghana - 210073  \n",
    "    Madhuri - 210568  \n",
    "    Shobhit Sharma - 210992\n",
    "(after taking assistance of Danish)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "7q-n5PaW0aA3"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "034a20511b174f39a2e69863015cdf01": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0a6afa0d17764312a4f24c781b5748a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e14eeb99a50b465c80a17436a786e48f",
      "placeholder": "​",
      "style": "IPY_MODEL_8bb28333ea744f2b94e5667ba58b92f7",
      "value": " 171M/171M [00:07&lt;00:00, 23.1MB/s]"
     }
    },
    "16191e415f6c43a19eab62d25ef4d259": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4229910ccd614a42b9176565f9ec473f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4419c8fd9caa4c91888055adc29dfbcb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7e73f282631b42c59e1483b65187399e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8970696ebd684e519ee031f4ad75a8dd",
       "IPY_MODEL_bd8cf7f6128849e895d4084d6c669add",
       "IPY_MODEL_0a6afa0d17764312a4f24c781b5748a5"
      ],
      "layout": "IPY_MODEL_16191e415f6c43a19eab62d25ef4d259"
     }
    },
    "8970696ebd684e519ee031f4ad75a8dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_034a20511b174f39a2e69863015cdf01",
      "placeholder": "​",
      "style": "IPY_MODEL_4419c8fd9caa4c91888055adc29dfbcb",
      "value": "model.safetensors: 100%"
     }
    },
    "8bb28333ea744f2b94e5667ba58b92f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bd8cf7f6128849e895d4084d6c669add": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4229910ccd614a42b9176565f9ec473f",
      "max": 171064452,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ce44b7e0ab4043a8be30b0496f16b5fb",
      "value": 171064452
     }
    },
    "ce44b7e0ab4043a8be30b0496f16b5fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e14eeb99a50b465c80a17436a786e48f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
